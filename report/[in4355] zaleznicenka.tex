\documentclass{llncs}

\begin{document}

\title{A Functional Approach To The Classification Problem}
\author{Zmicier Zaleznicenka, \#4134575}
\institute{%
Delft University of Technology \\ 
Faculty of Electronic Engineering, Mathematics and Computer Science \\
\email{D.V.Zhaleznichenka@student.tudelft.nl}
}

\maketitle

%-About the reports: you are expected to write around 10 pages, but not focus on quantity; instead focus on quality. A good report should explain the application domain, the existing approaches to building similar applications, a decomposition of the approach you have taken (e.g. high level architecture, design), the implementation with a focus on the functional programming techniques you have used and finally an evaluation of your approach. The deadline for the reports is Fri Nov 9.

\begin{abstract}
This report describes the project completed by the author during IN4355 Functional Programming course at TU Delft. The project goal was to implement two classification algorithms in a functional style using Python programming language.
\end{abstract}

\section{Introduction}

The research topic of the project discussed in this report is application of the functional programming techniques to the classification algorithms. The classification problem is one of the well-known statistical challenges and is being studied for several decades already. In statistical studies, classification means identifying a subset of categories from a category set to which a certain instance belongs. Classification is usually based on the existent training set with a number of instances already associated with the categories from a category set. Correlation between the instances and categories is defined by analyzing the quantifiable properties (features) of the subject instances\cite{WikiStatClass}.

Many different classification algorithms exist with each of them having different properties. These algorithms can themselves be classified to a number of categories, such as linear classifiers, decision trees, neural networks and more. In the scope of this project we will discuss the implementation of a naive Bayes classifier which is a rather simple representative of the linear classifiers and k-nearest neighbors algorithm belonging to the kernel estimation classifiers.

Classification algorithms are heavily used nowadays in many fields. The application domains where the classifiers are successfully applied are pattern recognition, natural languages processing, internet search, computer vision and more. The unstoppable growth of online data sets which is observed in recent years (also known as data deluge) forced the researchers to develop new efficient data mining techniques to successfully process these data sets. For data mining, classification algorithms are also often of utmost importance.

This report discusses the applicability of certain functional programming techniques to the implementation of two classification algorithms using Python programming language. The contribution of the completed project is the investigation on the applicability of Python as general-purpose functional programming language and the workability of functional programming techniques for data classification purposes.

The rest of this paper is organized as follows. In Section 2 we discuss the algorithms implemented in the project. Section 3 gives an insight into the existent approaches in implementing data classifiers used in industry and academia. Section 4 is devoted to the description of a project implementing two classifiers in a functional style. Section 5 contains the evaluation of the developed implementation and its comparison with the other existent software packages. In Section 6 the findings and conclusions are placed.

\section{Description of the implemented algorithms}

To reach the goals set for this project, it was decided to implement two well-known classification algorithms, namely naive Bayes (NB) and k-nearest neighbors (KNN). Both of these algorithms are relatively simple and are often used in different domains for the text classification, pattern recognition and other tasks. However, these classifiers have different properties. While NB algorithm is characterized as having low variance and high bias, KNN algorithm has high variance and low bias. From this it follows that NB should perform better for smaller data sets and KNN should be better for the large ones. Apart from this difference, there are some others that will not be covered in this document as it does not have a goal to compare the classifiers themselves. 

\subsection{Naive Bayes classifier}

Naive Bayes is considered to be the simplest classification algorithm possible. However, it has good execution speed and adequate error rate for many classification tasks. As its name implies, this classifier is based on the naive Bayes probabilistic model which assumes that the instance properties (features) are independent given class, that is, \(P(X|C) = \prod_{i=1}^nP(X_i|C)\). \cite{Rish}.

The implementation of a naive Bayes classifier usually consists of training and classifying stages. At the first stage, the instances from a training set are split into the classes. For these instances we know explicitly to which class each instance belongs. At the second stage, we take the features of the instance to be classified and find the probabilities of it belonging to all of the classes in order. The highest value of these probabilities is defined by the formula \(argmax(P(C)) * \prod_{i=1}^nP(X_i|C)\) and the instance is classified as belonging to the category having this highest probability value \cite{Manning}\cite{Rish}.

\subsection{K-nearest neighbors classifier}

K-nearest neighbors classifier (KNN) belongs to the set of kernel estimation algorithms and approaches the classification problem by locating the instances that are closest to the instance to be classified in a feature space\footnote{N-dimensional metric space for the instances having N features}\cite{WikiKNN}.

The algorithm idea is the following. Assume we have a set of classified instances defined by their feature vectors and located in the feature space. To classify an instance in the same space, we should find K of its nearest neighbors using the distance function and choose the most frequent class of these neighbors. The selection process is usually performed using the majority voting.

The idea to classify the instances based on the similarity with the instances lying in the close proximity in the feature space is significantly different from the classification based on Bayes probability model discussed earlier. For example, there is no explicit training stage in KNN algorithm. It is only required to store the feature vectors and the respective class labels in the memory before initiating the classification process.

Despite this algorithm seems to be rather straightforward and easy to implement, many researchers work on improving its properties for better performance on different data sets. The error rate of the algorithm depends significantly on the selection of distance and voting functions and many different implementation of them were designed by now.

For this project, Euclidean distance was used as distance function. However, one important adjustment was made to it to address the co-existence of continuous and discrete features in some datasets.  

Feature vectors in some datasets used to test the correctness of the algorithm shared both continuous (numerical) and discrete (categorical) features and there was a need in the distance function that would give adequate results for these mixed setups. To solve this problem, both continuous and discrete values were normalized prior to the classification. For continuous values, z-score normalization was applied. For different discrete values, 1 was taken as the distance value. For same discrete values, 0 was taken.

This normalization showed rather good results as can be seen in the evaluation section. However, these results can be improved further by optimizing the normalization process. The problem of normalization for mixed datasets was diligently studied in \cite{Suarez} and a better solution to it was proposed. The implementation of the mentioned approach for this project was left for future work. It is interesting, however, that this solution was proposed only recently. The authors of the mentioned paper report that not much work has been done beforehand to address the normalization of the mixed feature vectors which is rather important for data mining and other application domains and does not seem to be an overly complex problem.

\section{Existent approaches to building the classifiers}

Since the classification topic is important for many real-world tasks, there are many software packages that provide their customers a number of implementations for the classification algorithms. As the logic behind many of the classifiers is rather simple, implementations of most of such algorithms exist for virtually every programming language used in production. Classification algorithms are studied in academia and often used as the lab assignments. The popularity of these algorithms implies that there 

\section{Architecture and implementation}


\section{Evaluation}

\section{Conclusion}

\begin{thebibliography}{99}
\bibitem{Manning} C.D. Manning, P. Raghavan and H. Schutze. Introduction to information retrieval.  Cambridge University Press, 2008.  
\bibitem{Rish} I. Rish. An empirical study of the naive Bayes classifier. In \emph{Proceedings of IJCAI-01 workshop on Empirical Methods in AI}, pages 41--46, Sicily, Italy, 2001.
\bibitem{Suarez} M.M. Suarez-Alvarez, D. Pham, M.Y. Prostov and Y.I. Prostov. Statistical approach to normalization of feature vectors and clustering of mixed datasets. In \emph{Proc. R. Soc. A}, 2011. doi: 10.1098/rspa.2011.0704. 
\bibitem{WikiKNN} Wikipedia - K-nearest neighbor algorithm. \url{http://en.wikipedia.org/wiki/K-nearest\_neighbor\_algorithm}.
\bibitem{WikiStatClass} Wikipedia - Statistical classification. \url{http://en.wikipedia.org/wiki/Statistical\_classification}. 
\end{thebibliography}

\end{document}